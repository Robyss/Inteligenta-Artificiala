{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Source Dialect Identification\n",
    "The goal of the competition is to predict the native-dialect of a text based on its translation in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data_path = '../Lab 5/'\n",
    "train_data_df = pd.read_csv(os.path.join(data_path, 'train_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesarea datelor\n",
    "- extragem informatiile necesare din text\n",
    "- eliminam caracterele speciale si cifrele\n",
    "- eliminam cuvintele de legatura (stopwatches)\n",
    "- lematizam cuvintele in functie de limba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from simplemma import lemmatize\n",
    "from simplemma import lang_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords') # Download the stop words\n",
    "\n",
    "danish_stopwords = stopwords.words('danish')\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "italian_stopwords = stopwords.words('italian')\n",
    "dutch_stopwords = stopwords.words('dutch')\n",
    "german_stopwords = stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "def preprocesare_text(text):\n",
    "\n",
    "    text = re.sub(r'\\d-', '', text) # Eliminarea cifrelor\n",
    "    cuvinte = re.findall(r'\\w+', text) # Extragerea cuvintelor din text\n",
    "\n",
    "    # M-am gandit ca nu ar trebui sa eliminam stop words pentru ca poate exista \n",
    "    # un cuvant care este stop word in limba germana dar nu si in limba daneza.\n",
    "    # Astfel, vom identifica limba textului si vom elimina stop words \n",
    "    # din limba respectiva.\n",
    "\n",
    "    limba = lang_detector(text, lang=('da', 'de', 'es', 'it', 'nl'))\n",
    "    limba = limba[0][0]\n",
    "\n",
    "    # Eliminarea cuvintelor de legatura in functie de limba textului\n",
    "    if limba == 'da':\n",
    "        cuvinte = [cuvant for cuvant in cuvinte if cuvant not in danish_stopwords]\n",
    "    elif limba == 'de':\n",
    "        cuvinte = [cuvant for cuvant in cuvinte if cuvant not in german_stopwords]\n",
    "    elif limba == 'es':\n",
    "        cuvinte = [cuvant for cuvant in cuvinte if cuvant not in spanish_stopwords]\n",
    "    elif limba == 'it':\n",
    "        cuvinte = [cuvant for cuvant in cuvinte if cuvant not in italian_stopwords]\n",
    "    else:\n",
    "        cuvinte = [cuvant for cuvant in cuvinte if cuvant not in dutch_stopwords]\n",
    "\n",
    "    # Lematizarea cuvintelor in functie de limba\n",
    "    cuvinte = [lemmatize(cuvant, limba) for cuvant in cuvinte]\n",
    "\n",
    "    return cuvinte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mai multe detalii despre libraria [simplemma](https://adrien.barbaresi.eu/blog/simple-multilingual-lemmatizer-python.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicam functia de preprocesare intregului set de date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "\n",
    "# X = train_data_df.drop('label', axis=1)\n",
    "# y = train_data_df['label']\n",
    "\n",
    "# Aplicarea functiei de preprocesare asupra datelor\n",
    "# X['text'] = X['text'].apply(preprocesare_text)\n",
    "\n",
    "X = train_data_df['text'].apply(preprocesare_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Eliminarea cuvintelor care apar mai putin de 5 ori\n",
    "# cuvinte = []\n",
    "# for text in X['text']:\n",
    "#     cuvinte.extend(text)\n",
    "\n",
    "# cuvinte = pd.Series(cuvinte)\n",
    "# cuvinte = cuvinte.value_counts()\n",
    "# cuvinte = cuvinte[cuvinte > 5]\n",
    "\n",
    "# # Eliminarea cuvintelor care nu apar in lista de cuvinte\n",
    "# X['text'] = X['text'].apply(lambda x: [cuvant for cuvant in x if cuvant in cuvinte.index])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
