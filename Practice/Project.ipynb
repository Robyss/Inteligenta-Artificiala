{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Source Dialect Identification\n",
    "The goal of the competition is to predict the native-dialect of a text based on its translation in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data_path = '../Lab 5/'\n",
    "train_data_df = pd.read_csv(os.path.join(data_path, 'train_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesarea datelor\n",
    "Pentru a extrage informatiile necesare din text, facem urmatorii pasi:\n",
    "- eliminam caracterele speciale si cifrele\n",
    "- convertim litere mari la litere mici\n",
    "- eliminam cuvintele de legatura (stopwatches)\n",
    "- lematizam cuvintele in functie de limba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from simplemma import lemmatize\n",
    "from simplemma import lang_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords') # Download the stop words\n",
    "\n",
    "danish_stopwords = stopwords.words('danish')\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "italian_stopwords = stopwords.words('italian')\n",
    "dutch_stopwords = stopwords.words('dutch')\n",
    "german_stopwords = stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "def preprocesare_text(text):\n",
    "\n",
    "    text = re.sub(r'\\d-', '', text) # Eliminarea cifrelor\n",
    "    cuvinte = re.findall(r'\\w+', text) # Extragerea cuvintelor din text\n",
    "\n",
    "    # Convertirea cuvintelor in litere mici\n",
    "    cuvinte = [cuvant.lower() for cuvant in cuvinte]\n",
    "\n",
    "    # M-am gandit ca nu ar trebui sa eliminam stop words pentru ca poate exista \n",
    "    # un cuvant care este stop word in limba germana dar nu si in limba daneza.\n",
    "    # Astfel, vom identifica limba textului si vom elimina stop words \n",
    "    # din limba respectiva.\n",
    "\n",
    "    limba = lang_detector(text, lang=('da', 'de', 'es', 'it', 'nl'))\n",
    "    limba = limba[0][0]\n",
    "\n",
    "    # Eliminarea cuvintelor de legatura in functie de limba textului\n",
    "    if limba == 'da':\n",
    "        cuvinte = [cuvant for cuvant in cuvinte if cuvant not in danish_stopwords]\n",
    "    elif limba == 'de':\n",
    "        cuvinte = [cuvant for cuvant in cuvinte if cuvant not in german_stopwords]\n",
    "    elif limba == 'es':\n",
    "        cuvinte = [cuvant for cuvant in cuvinte if cuvant not in spanish_stopwords]\n",
    "    elif limba == 'it':\n",
    "        cuvinte = [cuvant for cuvant in cuvinte if cuvant not in italian_stopwords]\n",
    "    else:\n",
    "        cuvinte = [cuvant for cuvant in cuvinte if cuvant not in dutch_stopwords]\n",
    "\n",
    "    # Lematizarea cuvintelor in functie de limba\n",
    "    cuvinte = [lemmatize(cuvant, limba) for cuvant in cuvinte]\n",
    "\n",
    "    return cuvinte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mai multe detalii despre libraria [simplemma](https://adrien.barbaresi.eu/blog/simple-multilingual-lemmatizer-python.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicam functia de preprocesare intregului set de date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ preprocesare_text(text) for text in train_data_df['text'] ] # Pentru a nu rula preprocesarea de fiecare data\n",
    "y = list(train_data_df['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Eliminarea cuvintelor care apar mai putin de 5 ori\n",
    "# cuvinte = []\n",
    "# for text in X['text']:\n",
    "#     cuvinte.extend(text)\n",
    "\n",
    "# cuvinte = pd.Series(cuvinte)\n",
    "# cuvinte = cuvinte.value_counts()\n",
    "# cuvinte = cuvinte[cuvinte > 5]\n",
    "\n",
    "# # Eliminarea cuvintelor care nu apar in lista de cuvinte\n",
    "# X['text'] = X['text'].apply(lambda x: [cuvant for cuvant in x if cuvant in cuvinte.index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impartirea datelor in train si test\n",
    "Pentru a imparti datele in mod optim se va folosi din libraria `sklearn.model_selection` pachetul `train_train_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33256 8314 33256 8314\n"
     ]
    }
   ],
   "source": [
    "# Impartirea datei in train si test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Vectorizarea cuvintelor\u001b[39;00m\n\u001b[1;32m      5\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer(analyzer\u001b[39m=\u001b[39mpreprocesare_text)\n\u001b[0;32m----> 6\u001b[0m X_train \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[1;32m      7\u001b[0m X_test \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform(X_test)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1210\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:108\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    106\u001b[0m     doc \u001b[39m=\u001b[39m decoder(doc)\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     doc \u001b[39m=\u001b[39m analyzer(doc)\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn [35], line 5\u001b[0m, in \u001b[0;36mpreprocesare_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocesare_text\u001b[39m(text):\n\u001b[0;32m----> 5\u001b[0m     text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49msub(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39md-\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, text) \u001b[39m# Eliminarea cifrelor\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     cuvinte \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mfindall(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m'\u001b[39m, text) \u001b[39m# Extragerea cuvintelor din text\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39m# Convertirea cuvintelor in litere mici\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/re.py:209\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msub\u001b[39m(pattern, repl, string, count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39;49msub(repl, string, count)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorizarea cuvintelor\n",
    "vectorizer = CountVectorizer(analyzer=preprocesare_text)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Antrenarea modelului\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Testarea modelului\n",
    "predictions = model.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
